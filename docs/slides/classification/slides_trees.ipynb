{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Introduction to Machine Learning\n",
    "## Non-probabilistic Models for Classification\n",
    "### July 2nd, 2019\n",
    "### Instructors: Melanie Pradier Fernandez (Havard), Weiwei Pan (Harvard), Javier Zazo Ruiz (Harvard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize a ellipitical decision boundary?\n",
    "\n",
    "<img src=\"./fig/fig1.png\" style='height:300px;'>\n",
    "\n",
    "We can say that the decision boundary is given by a ***quadratic function*** of the input:\n",
    "$$\n",
    "w_1x^2_1 + w_2x^2_2 + w_3 = 0\n",
    "$$\n",
    "We say that we can fit such a decision boundary using logistic regression with degree 2 polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize an arbitrary complex decision boundary?\n",
    "\n",
    "<img src=\"./fig/fig2.png\" style='height:300px;'>\n",
    "\n",
    "It's not easy to think of a function $g(x)$ can capture this decision boundary.\n",
    "\n",
    "**GOAL:** Find models that can capture *arbitrarily complex* decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Two Interpretations of Decision Trees\n",
    "People in every walk of life have long been using ***decision trees*** (flow charts) for differentiating between classes of objects and phenomena:\n",
    "<img src=\"./fig/fig3.png\" alt=\"\" style=\"height: 300px;\"/>\n",
    "Every flow chart tree corresponds to a partition of the input space by axis aligned lines or (hyper) planes.\n",
    "\n",
    "Conversely, every such partition can be written as a flow chart tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting Decision Trees\n",
    "\n",
    "We fit decision trees to training data using a **greedy algorithm**: we perform one 'best cut' at a time:\n",
    "1. Start with an empty decision tree (undivided feature space)\n",
    "2. Choose the 'optimal' predictor on which to split and choose the 'optimal' threshold value for splitting.\n",
    "3. Recurse on on each new node until some stopping condition is met\n",
    "\n",
    "Typically, we measure optimality by the **purity** (in terms of observed clases) of each region defined by the tree.\n",
    "\n",
    "We usually stop until a pre-defined maximum depth is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Tree Implementation in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#import the decision tree model\n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTree\n",
    "\n",
    "#instantiate a tree of depth 5\n",
    "tree = DecisionTree(max_depth=5, criterion=’gini’)\n",
    "\n",
    "#fit your tree to data\n",
    "tree.fit(x_train, y_train)\n",
    "\n",
    "#predict new labels using the fitted tree\n",
    "tree.predcit(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## For Practitioners\n",
    "\n",
    "A decision tree model, as implemented in `sklearn`, has several hyper-parameters. Two important ones being:\n",
    "\n",
    "1. stopping condition (usually the max depth, but can also be minimum number of observations per region defined by tree)\n",
    "2. splittin condition (what criteria of purity, in terms of observed classes, used to determine an 'optimal' split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Compare Decision Tree to Logistic Regression \n",
    "\n",
    "**Application:** Remote sensing, i.e. classification of locations in satellite images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep vs Shallow Trees: The Bias Variance Trade Off\n",
    "\n",
    "<img src=\"./fig/fig4.png\" alt=\"\" style=\"height: 500px;\"/>\n",
    "\n",
    "What is the advantages and disadvantages of shallow trees versus deep trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep vs Shallow Trees: The Bias Variance Trade Off\n",
    "If you randomly draw 50 sample from our toy data with non-linear boundary and fit a decision tree on each sample individually, their decision boundaries will look like the following:\n",
    "\n",
    "<img src=\"./fig/fig5.png\" alt=\"\" style=\"height: 300px;\"/>\n",
    "\n",
    "What does this say about the variance of very deep trees? How does this impact the generalization of deep trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging to the Rescue\n",
    "\n",
    "Yesterday, we saw that when we created ensembles of models with high variance and averaged their predictions, then the averaged prediction generalized better. This method is called **bagging**.\n",
    "\n",
    "Just as we can ensemble polynomials, we can ensemble deep decision trees. Applying bagging to decision trees gives us a model called **random forest**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ensemble Methods: Bagging (Random Forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests\n",
    "As we've seen:\n",
    "\n",
    "1. a shallow decision tree is unable to capture complex decision boundaries (high bias)\n",
    "2. a deep decision tree is overly sensitive to the noise in the data leading to overfitting (high variance)\n",
    "\n",
    "A compromise for reducing variance is to fit a large number of sensitive models (deep trees) on the training data and then average the results (reducing the variance).\n",
    "\n",
    "<img src=\"./fig/fig6.jpg\" alt=\"\" style=\"height: 300px;\"/>\n",
    "\n",
    "A **random forest** is the 'averaged model' of collection of deep decision trees each learned on a subset of the training data (each branch in each tree is also trained on a randomized set of input dimensions to reduce correlation between trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forest Implementation in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#import the random forest model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#instantiate a forest of with 1000 trees, each of depth 1000\n",
    "forest = RandomForestClassifier(n_estimators=1000, max_depth=1000)\n",
    "\n",
    "#fit your forest to data\n",
    "forest.fit(x_train, y_train)\n",
    "\n",
    "#predict new labels using the fitted forest\n",
    "forest.predcit(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## For Practitioners:\n",
    "\n",
    "A random forest, as implemented in `sklearn`, has a number of hyper-parameters that effects model performance. Some important hyper-parameters are:\n",
    "\n",
    "1. number of trees in the forest `n_estimators`\n",
    "2. max depth of trees `max_depth`\n",
    "3. hyper-parameters that determines the subsets of input dimensions to learn each branch and the subsets of training data to learn each tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Compare Deep Trees to Random Forests\n",
    "\n",
    "**Application:** Remote sensing, i.e. classification of locations in satellite images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# kNN Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## kNN for Classification\n",
    "As we mentioned before, you can use the k-Nearest-Neighbours model to perform classification instead of regression.\n",
    "\n",
    "\n",
    "<img src=\"./fig/fig7.png\" alt=\"\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Boundaries of kNN Classifiers\n",
    "\n",
    "What does the decision boundaries of kNN classifiers look like? Can kNN classifiers capture non-linear decision boundaries?\n",
    "<table>\n",
    "    <tr><td><img src=\"./fig/fig1.png\" style='height:300px;'></td><td><img src=\"./fig/fig2.png\" style='height:300px;'></td>\n",
    "    <tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## kNN Classification Implementation in `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#import the kNN classification model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#instantiate a kNN classifier with 3 neighbors\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "#fit your classifier to data\n",
    "knn.fit(x_train, y_train)\n",
    "\n",
    "#predict new labels using the fitted classifier\n",
    "knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An Embarassment of Riches\n",
    "\n",
    "Now we have seen four types of classifiers: logistic regression, decision tree, random forest, kNN. Each type can be customized in many ways (e.g. we can choose many different polynomials for the logistic regression boundary)\n",
    "\n",
    "***Question:*** which model should we use?\n",
    "\n",
    "***Answer:*** your choice of model should depend on the task and the dataset. You must\n",
    "1. choose models based on sensible evaluation metrics\n",
    "2. choose models using proper data set splitting procedure (train/validation/test)\n",
    "3. choose models that best solves your real-life task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Compare All Classification Models on a Real Task\n",
    "\n",
    "**Application:** Automated cancer diagnosis - classify biopsy results as cancerous or non-cancerous."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
