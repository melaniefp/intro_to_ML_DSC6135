{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from sklearn import preprocessing, linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering\n",
    "\n",
    "\n",
    "### Machine Learning and Computational Statistics (DSC6135)\n",
    "\n",
    "### Instructors: Weiwei Pan (Harvard), Javier Zazo (Harvard), Melanie F. Pradier (Harvard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Supervised Learning\n",
    "\n",
    "* **Unsupervised Learning**\n",
    "\n",
    "<img src=\"figs/unsupervised.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Some Application Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figs/ap1.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figs/ap2.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figs/ap3.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"figs/ap4.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to cluster these points?\n",
    "\n",
    "<img src=\"figs/points1.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to cluster these points?\n",
    "\n",
    "<img src=\"figs/points2.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Key Questions    \n",
    "    \n",
    "* How many clusters?\n",
    "\n",
    "<img src=\"figs/howmanyclusters.jpg\" width=\"80%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Key Questions\n",
    "\n",
    "* What shape for the clusters?\n",
    "\n",
    "<img src=\"figs/shapes1.png\" width=\"60%\">\n",
    "<img src=\"figs/shapes2.png\" width=\"60%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K-Means\n",
    "\n",
    "* **Input**:\n",
    "    * $N$ datapoints: $x_1, x_2, \\ldots, x_N$\n",
    "    * parameters $K$ (number of clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* **Goals of K-means**:\n",
    "    1. Assign each datapoint to one of $K$ clusters<br> (*Assumption*: clusters are exclusive)\n",
    "    2. Minimize Euclidean distance from datapoints to cluster centers<br>\n",
    "    (*Assumption*: isotropic Euclidean - all features weighted equally - is a good metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-means output:\n",
    "\n",
    "* Centroid vectors (one per cluster $k=1,\\ldots,K$)\n",
    "\n",
    "    $$ \\mu_k = [\\mu_{k1}, \\mu_{k2}, \\ldots, \\mu_{kD}] $$\n",
    "    \n",
    "* Cluster Assignments (one per datapoint $n=1,\\ldots,N$)\n",
    "\n",
    "    $$ z_n = [z_{n1}, z_{n2}, \\ldots, z_{nK}] = [0\\, 1\\, 0\\, 0\\,  0] $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### K-means Optimization problem\n",
    "\n",
    "$$\\min_{z,\\mu} \\sum^N_{n=1} \\sum^K_{k=1} z_{nk} \\, \\mathrm{distance}(x_n,\\mu_k)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\mathrm{distance}(x_n,\\mu_k) = ||x_n - \\mu_k ||^2$$\n",
    "\n",
    "Directly optimizing this expression is problematic!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iterative Algorithm (Expectation-Maximization)\n",
    "\n",
    "* Initialize cluster means randomly\n",
    "\n",
    "* Repeat until converged\n",
    "\n",
    "<img src=\"figs/alg1.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Iterative Algorithm (Expectation-Maximization)\n",
    "\n",
    "* Initialize cluster means randomly\n",
    "\n",
    "* Repeat until converged\n",
    "\n",
    "<img src=\"figs/alg2.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Demo!\n",
    "\n",
    "https://www.naftaliharris.com/blog/visualizing-k-means-clustering/\n",
    "\n",
    "* For the same data and number of clusters, can you find different clustering configurations?\n",
    "\n",
    "* What does this mean about the objective function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "A different demo:\n",
    "\n",
    "http://stanford.edu/class/ee103/visualizations/kmeans/kmeans.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interesting Notes\n",
    "\n",
    "* K-means boundaries are linear\n",
    "\n",
    "<img src=\"figs/kmeans.png\" width=\"80%\">\n",
    "\n",
    "* Each step in EM-algorithm leads to equal or lower cost than previous one\n",
    "\n",
    "* Initialization important (leads to different solutions)\n",
    "* Use cost to decide among multiple runs of $K$-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# K-means algorithm from scratch (doing Expectation-Maximization)\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "\n",
    "def find_clusters(X, n_clusters, rseed=2):\n",
    "    # 1. Randomly choose clusters\n",
    "    rng = np.random.RandomState(rseed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters]\n",
    "    centers = X[i]\n",
    "    \n",
    "    while True:\n",
    "        # 2a. Get cluster assignments based on closest center\n",
    "        assignments = pairwise_distances_argmin(X, centers)\n",
    "        \n",
    "        # 2b. Find new centers from means of points\n",
    "        new_centers = np.array([X[assignments == i].mean(0)\n",
    "                                for i in range(n_clusters)])\n",
    "        \n",
    "        # 2c. Check for convergence\n",
    "        if np.all(centers == new_centers):\n",
    "            break\n",
    "        centers = new_centers\n",
    "    \n",
    "    return centers, assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to initialize the centroids\n",
    "\n",
    "* Step 1: choose an example uniformly at random as first centroid\n",
    "* Repeat for k = 2, 3, â€¦ K: \n",
    "    - Choose example based on distance from nearest centroid:\n",
    "        $$\n",
    "        P(\\mu_k) \\propto \\min_{1,2,\\ldots,k-1} \\text{dist}(x_n,\\mu_j)\n",
    "        $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* This procedure initializes clusters far from each other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* There are some theoretical guarantees of worst-case performance...\n",
    "* ... but the problem is hard!\n",
    "\n",
    "How do we address this difficulties?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Multiple initializations --> choose based on some score evaluation / metrics\n",
    "\n",
    "* There are no guarantees that your cluster results will be good with a single one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multiple initializations: scoring\n",
    "\n",
    "<img src=\"figs/cluster_init.png\" width=\"80%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we evaluate the score?\n",
    "\n",
    "* Many possibile metrics: based on true labels, distance considerations between clusters...\n",
    "* In practical, we use shiloutte coefficient:\n",
    "\n",
    "<img src=\"figs/shilhouette_score.png\" width=\"80%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How to choose number of clusters $K$?\n",
    "\n",
    "<img src=\"figs/varyingK.png\" width=\"80%\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Can we use the cost function?\n",
    "\n",
    "<img src=\"figs/elbow0.jpg\" width=\"70%\">\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* We should not!! Instead, regularization\n",
    "\n",
    "$$\\textrm{loss function} = \\textrm{cost} + \\lambda \\, \\textrm{penalty}(K)$$\n",
    "\n",
    "* Or... based on elbow:\n",
    "\n",
    "<img src=\"figs/elbow.jpg\" width=\"70%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Disadvantages of $K$-means\n",
    "\n",
    "* Hard assignments: clusters are exclusive\n",
    "\n",
    "\n",
    "* Weight equally each feature (isotropic Euclidean distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Improving $K$-means:\n",
    "\n",
    "1. Assign each datapoint to one of $K$ clusters<br> (*Assumption*: clusters are exclusive)<br>\n",
    "    **Improvement**: soft-probabilistic assignments<br>\n",
    "    \n",
    "    \n",
    "2. Minimize Euclidean distance from datapoints to cluster centers<br>\n",
    "    (*Assumption*: isotropic Euclidean - all features weighted equally - is a good metric)<br>\n",
    "    **Improvement**: model cluster covariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```\n",
    "gmm = GMM(n_components=4, covariance_type='full')\n",
    "```\n",
    "<img src=\"figs/GMM.png?1\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multivariate Gaussian\n",
    "\n",
    "<img src=\"figs/multivariate_gaussian.png?1\" width=\"70%\">\n",
    "\n",
    "* Probability density function:\n",
    "$$f_Y(x)=\\frac{1}{\\sqrt{(2\\pi)^n|\\boldsymbol\\Sigma|}}\n",
    "\\exp\\left(-\\frac{1}{2}({x}-{\\mu})^T{\\boldsymbol\\Sigma}^{-1}({x}-{\\mu})\n",
    "\\right)$$\n",
    "\n",
    "* Mean: $\\mu \\in\\mathbb{R}^F$\n",
    "* Covariance: $\\Sigma \\in\\mathbb{R}^{F\\times F}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Covariance models\n",
    "\n",
    "<img src=\"figs/cov.png?1\" width=\"80%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parameters for Gaussian Mixture Models:\n",
    "\n",
    "* mean vectors --> one per cluster $k$.\n",
    "$$\\mu_k=[\\mu_{k1},\\mu_{k2},\\ldots,\\mu_{kF}]$$\n",
    "\n",
    "* covariance matrix --> one per cluster $k$.\n",
    "$$ \\Sigma_k \\in \\mathbb{R}^{F\\times F}, \\qquad\\forall k\\in \\{1,\\ldots,K\\} $$\n",
    "\n",
    "* soft assignments --> one per example $n$ in $1,\\ldots, N$\n",
    "$$ r_n = [r_{n1}, r_{n2},\\ldots, r_{nK}] \\qquad\\forall n\\in \\{1,\\ldots,N\\} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "TALK about probabilistic perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Multivariate Gaussian parameter estimation Training\n",
    "\n",
    "* How to fit the parameters of a multivariate Gaussian from data?\n",
    "\n",
    "$$ X = {x_{1}, x_2, \\dots, x_N}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Maybe with a log-maximum likelihood estimate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Mean is the average of the samples:\n",
    "\n",
    "$$ \\mu = \\frac{1}{N} \\sum_{i=1}^N x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* The covariance matrix:\n",
    "    \n",
    "$$\\Sigma = \\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu) (x_i - \\mu)^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model training\n",
    "\n",
    "* We can estimate the parameters of multivariate Gaussians easily...\n",
    "\n",
    "* ... but we don't know to which cluster each data point belongs to.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* We can, once more, maximize log-likelihood!\n",
    "\n",
    "$$\\max_{\\mu_k, \\Sigma_k} \\quad \\sum_{i=1}^{N}\\sum_{k=1}^{K} \\log p(x_i, z_{ik} =1 | \\mu_k, \\Sigma_k) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* How do we optimize?\n",
    "\n",
    "*  We don't have cluster assignments, and we don't have mean and covariance estimates --> but we will use soft assignments!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Expectation - Maximization\n",
    "\n",
    "* We repeat the following procedure until convergence\n",
    "\n",
    "* On each step, we assume the other variables fixed.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "1. E-step: Update soft assignments $r$ (and normalize):\n",
    "\n",
    "$$ r_{ik} \\propto p(x_i, z_k =1 | \\mu_k, \\Sigma_k) \\quad \\forall k, i$$ \n",
    "\n",
    "2. M-step: Update means and covariances:\n",
    "\n",
    "$$ \\mu = \\frac{1}{N} \\sum_{i=1}^N r_{ik} x_i$$\n",
    "\n",
    "$$\\Sigma = \\frac{1}{N} \\sum_{i=1}^N  r_{ik} (x_i - \\mu) (x_i - \\mu)^T$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $K$-means is a GMM with:\n",
    "\n",
    "* hard assignments --> probabilities $r$ are one-hot encodings.\n",
    "* spherical covariances\n",
    "\n",
    "$$ \\Sigma_k = \\lim_{\\sigma\\rightarrow 0}\\sigma^2 \\left[\\begin{array}{ccc}\n",
    "1 & 0 & 0\\\\\n",
    "0 & 1 & 0\\\\\n",
    "0 & 0 & 1\n",
    "\\end{array} \\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
