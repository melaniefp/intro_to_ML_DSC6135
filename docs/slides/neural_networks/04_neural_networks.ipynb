{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Introduction to Machine Learning\n",
    "## Neural Networks\n",
    "### July 3rd, 2019\n",
    "### Instructors: Melanie Pradier Fernandez (Havard), Weiwei Pan (Harvard), Javier Zazo Ruiz (Harvard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Review of the Geometry of Logistic Regression \n",
    "In **logistic regression**, we model the probability of an input $\\mathbf{x}$ being labeled '1' as a function of its distance from the hyperplane parametrized by $\\mathbf{w}$\n",
    "<img src=\"./fig/fig0.png\" style='height:300px;'>\n",
    "That is, we model $p(y=1 | \\mathbf{w}, \\mathbf{x}) = \\sigma(\\mathbf{w}^\\top \\mathbf{x})$. Where $\\mathbf{w}^\\top \\mathbf{x}=0$ is the equation of the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize a ellipitical decision boundary?\n",
    "\n",
    "<img src=\"./fig/fig1.png\" style='height:300px;'>\n",
    "\n",
    "We can say that the decision boundary is given by a ***quadratic function*** of the input:\n",
    "$$\n",
    "w_1x^2_1 + w_2x^2_2 + w_3 = 0\n",
    "$$\n",
    "We say that we can fit such a decision boundary using logistic regression with degree 2 polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize an arbitrary complex decision boundary?\n",
    "\n",
    "<img src=\"./fig/fig2.png\" style='height:300px;'>\n",
    "\n",
    "It's not easy to think of a function $g(x)$ can capture this decision boundary.\n",
    "\n",
    "**GOAL:** Find models that can capture *arbitrarily complex* decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Decision Boundaries of Different Classifiers\n",
    "\n",
    "Last time, we proposed three types of models that can capture arbitrarily complex decision boundaries: \n",
    "1. decision tree\n",
    "2. random forest\n",
    "3. kNN classifier\n",
    "\n",
    "<img src=\"./fig/fig3.png\" style='height:300px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An Embarassment of Riches\n",
    "\n",
    "Now we have seen four types of classifiers: logistic regression, decision tree, random forest, kNN. Each type can be customized in many ways (e.g. we can choose many different polynomials for the logistic regression boundary)\n",
    "\n",
    "***Question:*** which model should we use?\n",
    "\n",
    "***Answer:*** your choice of model should depend on the task and the dataset. You must\n",
    "1. choose models based on sensible evaluation metrics\n",
    "2. choose models using proper data set splitting procedure (train/validation/test)\n",
    "3. choose models that best solves your real-life task!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Compare All Classification Models on a Real Task\n",
    "\n",
    "**Application:** Automated cancer diagnosis - classify biopsy results as cancerous or non-cancerous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparison of Classifiers: Computational Concerns\n",
    "In addition to considering how well a model solves our real-life task, we also need to consider how easily we can apply a model to large amounts of data - this is called ***scalability***. \n",
    "\n",
    "Specifically, we want to know how easily we can train a model and how easily we can compute a prediction.\n",
    "\n",
    "**QUESTION:** What are the advantages and disadvantages of each type of classifier with respect to scalability?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How would you parametrize an arbitrary complex decision boundary?\n",
    "\n",
    "<img src=\"./fig/fig2.png\" style='height:300px;'>\n",
    "\n",
    "It's not easy to think of a function $g(x)$ can capture this decision boundary.\n",
    "\n",
    "**REVISED GOAL:** Find models that can capture *arbitrarily complex* decision boundaries **and** are fast to train as well as effecient for computing predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Approximating Arbitrarily Complex Decision Boundaries\n",
    "\n",
    "Given an exact parametrization, we could learn the functional form, $g$, of the decision boundary directly. \n",
    "\n",
    "However, assuming an exact form for $g$ is restrictive. \n",
    "\n",
    "Rather, we can build increasingly good approximations, $\\widehat{g}$, of $g$ by composing simple functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Neural Network?\n",
    "\n",
    "**Goal:** build a good approximation $\\widehat{g}$ of a complex function $g$ by composing simple functions.\n",
    "\n",
    "For example, let the following picture represents $f\\left(\\sum_{i}w_ix_i\\right)$, where $f$ is a non-linear transform:\n",
    "\n",
    "<img src=\"./fig/fig4.png\" style='height:200px;'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks as Function Approximators\n",
    "\n",
    "Then we can define the approximation $\\widehat{g}$ with a graphical schema representing a complex series of compositions and sums of the form, $f\\left(\\sum_{i}w_ix_i\\right)$\n",
    "\n",
    "<img src=\"./fig/fig5.png\" style='height:300px;'>\n",
    "\n",
    "This is a ***neural network***. We denote the weights of the neural network collectively by $\\mathbf{W}$.\n",
    "The non-linear function $f$ is called the ***activation function***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Translate Graphical Representations into Functional Ones\n",
    "\n",
    "Translate the following graphical representation of a neural network into a functional form, $g(x_1, x_2) = ?$ \n",
    "<img src=\"./fig/fig7.png\" style='height:100px;'>\n",
    "Use the following labels:\n",
    "1. the input nodes $x_1$ and $x_2$\n",
    "2. the hidden nodes $h_1$ and $h_2$\n",
    "3. the output node $y$\n",
    "4. the weight from $x_i$ to $h_j$ as $w^i_j$\n",
    "5. the weight from $h_j$ to $y$ as $w^j$\n",
    "6. the activation function $g(x) = e^{-0.5x^2}$\n",
    "\n",
    "**Bonus:** write the functional form in vector notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Flexible Framework for Function Approximation\n",
    "\n",
    "\n",
    "<img src=\"./fig/fig6.png\" style='height:500px;'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common Choices for the Activation Function\n",
    "\n",
    "<img src=\"./fig/fig8.png\" style='height:500px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Neural Networks for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks Regression\n",
    "\n",
    "**Data:** features `x_train`, real-valued labels `y_train`\n",
    "\n",
    "**Probabilistic Model:** `y_train` $= g_\\mathbf{W}($ `x_train` $) + \\epsilon,\\quad$ $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, where $g_\\mathbf{W}$ is a neural network with parameters $\\mathbf{W}$.\n",
    "\n",
    "**Training Objective:** find $\\mathbf{W}$ to maximize the likelihood of our data. This is equivalent to minimizing the Mean Square Error,\n",
    "$$\n",
    "\\max_{\\mathbf{W}}\\, \\mathrm{MSE}(\\mathbf{W}) = \\frac{1}{N}\\sum^N_{n=1} \\left(y_n - g_\\mathbf{W}(x_n)\\right)^2\n",
    "$$\n",
    "\n",
    "**Optimizing the Training Objective:** For linear regression (when $g_\\mathbf{W}$ is a linear function), we computed the gradient of the MSE with respective to the model parameters $\\mathbf{W}$, set it equal to zero and solved for the optimal $\\mathbf{W}$ analytically. \n",
    "\n",
    "Can we do the same when $g_\\mathbf{W}$ is a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Optimizing Neural Networks\n",
    "\n",
    "For the small neural network in the previous exercise, compute the gradient $\\nabla_{\\mathbf{W}}\\,\\mathrm{MSE}(\\mathbf{W})$. Can you analytically solve for the optimal parameters $\\mathbf{W}$? Is the training objective convex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent for Training Neural Networks: BackProp\n",
    "The intuition behind various flavours of gradient descent  is as follows:\n",
    "\n",
    "<img src=\"./fig/fig9.pdf\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent: the Algorithm\n",
    "1. start at random place: $W_0\\leftarrow \\textbf{random}$\n",
    "\n",
    "2. until (stopping condition satisfied):\n",
    "\n",
    "  a. compute gradient: \n",
    "     gradient = $\\nabla$ loss\\_function($W_{t}$)\n",
    "\n",
    "  b. take a step in the negative gradient direction: \n",
    "     $W_{t+1} \\leftarrow W_{t}$ - eta * gradient\n",
    "\n",
    "Here *eta* is called the ***learning rate***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementing Neural Networks in `python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `keras`: a Python Library for Neural Networks\n",
    "\n",
    "`keras` is a `python` library that provides intuitive api's for build neural networks quickly. \n",
    "\n",
    "``` python\n",
    "#keras model for feedforward neural networks\n",
    "from keras.models import Sequential\n",
    "\n",
    "#keras model for layers in feedforward networks\n",
    "from keras.layers import Dense\n",
    "\n",
    "#keras model for optimizing training objectives\n",
    "from keras import optimizers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a Neural Network for Regression in `keras`\n",
    "\n",
    "`keras` is a `python` library that provides intuitive api's for build neural networks quickly.\n",
    "\n",
    "``` python\n",
    "#instantiate a feedforward model\n",
    "model = Sequential()\n",
    "\n",
    "#add layers sequentially\n",
    "\n",
    "#input layer: 2 input dimensions\n",
    "model.add(Dense(2, input_dim=2, activation='relu', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "#hidden layer: 2 nodes\n",
    "model.add(Dense(2, activation='relu', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "\n",
    "#output layer: 1 output dimension\n",
    "model.add(Dense(1, activation='relu', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "\n",
    "#configure the model: specify training objective and training algorithm\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='mean_squared_error')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a Neural Network in `keras`\n",
    "\n",
    "``` python\n",
    "#fit the model and return the mean squared error during training\n",
    "history = model.fit(X_train, Y_train, batch_size=20, shuffle=True, epochs=100, verbose=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Your NN: Optimization Choices Matter\n",
    "<img src=\"./fig/fig10.jpg\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monitoring Neural Network Training\n",
    "\n",
    "Visualize the mean square error over the training, this is called the training ***trajectory***.\n",
    "\n",
    "``` python\n",
    "#fit the model and return the mean squared error during training\n",
    "history = model.fit(X_train, Y_train, batch_size=20, shuffle=True, epochs=100, verbose=0)\n",
    "\n",
    "# Plot the loss function and the evaluation metric over the course of training\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.plot(np.array(history.history['mean_squared_error']), color='blue', label='training accuracy')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnosing Issues with the Trajectory\n",
    "If this is your objective function during training, what can you conclude about your step-size?\n",
    "<img src=\"./fig/fig13.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnosing Issues with the Trajectory\n",
    "If this is your objective function during training, what can you conclude about your step-size?\n",
    "<img src=\"./fig/fig14.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Diagnosing Issues with the Trajectory\n",
    "If this is your objective function during training, what can you conclude about your step-size?\n",
    "<img src=\"./fig/fig15.png\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Compare Neural Network Regression to Polynomial Regression\n",
    "\n",
    "Compare a neural network regression to a polynomial regression. Which model do you think is \"better\" and for what kind of tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Neural Networks for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data:** features `x_train`, real-valued labels `y_train`\n",
    "\n",
    "**Probabilistic Model:** `y_train` $\\sim \\text{Bernoulli}(\\sigma(g_\\mathbf{W}($ `x_train` $))$, where $g_\\mathbf{W}$ is a neural network with parameters $\\mathbf{W}$, and $\\sigma(z) = \\frac{1}{1+e^{-z}}$.\n",
    "\n",
    "**Training Objective:** find $\\mathbf{W}$ to maximize the likelihood of our data. This is equivalent to minimizing the ***binary cross entropy*** or ***log loss***,\n",
    "$$\n",
    "\\max_{\\mathbf{W}}\\, \\mathrm{CrossEnt}(\\mathbf{W}) =\\sum^N_{n=1} y_n \\log( g_\\mathbf{W}(x_n)) + (1 - y_n) \\log\\left(1 - g_\\mathbf{W}(x_n)\\right)\n",
    "$$\n",
    "\n",
    "**Optimizing the Training Objective:** Since this objective is not convex and finding the zero's of the gradient is intractable, we will use gradient descent to find a \"optimal\" set of parameters $\\mathbf{W}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a Neural Network for Classification in `keras`\n",
    "\n",
    "``` python\n",
    "#instantiate a feedforward model\n",
    "model = Sequential()\n",
    "\n",
    "#add layers sequentially\n",
    "\n",
    "#input layer: 2 input dimensions\n",
    "model.add(Dense(2, input_dim=2, activation='relu', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "#hidden layer: 2 nodes\n",
    "model.add(Dense(2, activation='relu', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "\n",
    "#output layer: 1 output dimension\n",
    "model.add(Dense(1, activation='sigmoid', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "\n",
    "#configure the model: specify training objective and training algorithm\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='binary_crossentropy')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Compare Neural Network Classification to Other Classifiers\n",
    "\n",
    "Compare a neural network classifier to other classifiers. For what task is a neural network the most appropriate model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bias Variance Trade-off: for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization Error and Bias/Variance\n",
    "Complex models have ***low bias*** -- they can model a wide range of functions, given enough samples.\n",
    "\n",
    "But complex models like neural networks can use their 'extra' capacity to explain non-meaningful features of the training data that are unlikely to appear in the test data (i.e. noise). These models have ***high variance*** -- they are very sensitive to small changes in the data distribution, leading to drastic performance decrease from train to test settings.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"./fig/fig11.png\" style=\"width: 350px;\" align=\"center\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./fig/fig12.png\" style=\"width: 350px;\" align=\"center\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    "A way to prevent overfitting is to reduce the capacity of the model, thereby limiting the kinds of functions they can model. This **increases bias, but reduces variance**:\n",
    "1. **$\\ell_1$, $\\ell_2$ weight regularization** - adding a term to the loss function that penalizes the $\\ell_1$-norm (sum of absolute values) or the $\\ell_2$-norm (sum of squares) of the weights. This prevents the network from learning extremely squiggly functions.\n",
    "``` python\n",
    "from keras import regularizers\n",
    "model.add(Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "```\n",
    "\n",
    "2. **Dropout** - randomly zeroing out weights during training. This prevents the hidden nodes from \"over specializing\" or \"memorizing\" certain data points.\n",
    "``` python\n",
    "from keras.layers import Dropout,\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
