{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Introduction to Machine Learning\n",
    "## Neural Networks\n",
    "### July 8rd, 2019\n",
    "### Instructors: Melanie Fernandez Pradier (Havard), Weiwei Pan (Harvard), Javier Zazo Ruiz (Harvard)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Can we build a model that is arbitrarily flexible AND scalable?\n",
    "\n",
    "<img src=\"./fig/fig2.png\" style='height:300px;'>\n",
    "\n",
    "\n",
    "**REVISED GOAL:** Find models that can capture *arbitrarily complex* trends or decision boundaries **and** are fast to train as well as efficient for computing predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Neural Network?\n",
    "\n",
    "**Goal:** build a good approximation $\\widehat{g}$ of a complex function $g$ by composing simple functions.\n",
    "\n",
    "For example, let the following picture represents $f\\left(\\sum_{i}w_ix_i\\right)$, where $f$ is a non-linear transform:\n",
    "\n",
    "<img src=\"./fig/fig4.png\" style='height:200px;'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks as Function Approximators\n",
    "\n",
    "Then we can define the approximation $\\widehat{g}$ with a graphical schema representing a complex series of compositions and sums of the form, $f\\left(\\sum_{i}w_ix_i\\right)$\n",
    "\n",
    "<img src=\"./fig/fig5.png\" style='height:300px;'>\n",
    "\n",
    "This is a ***neural network***. We denote the weights of the neural network collectively by $\\mathbf{W}$.\n",
    "The non-linear function $f$ is called the ***activation function***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Common Choices for the Activation Function\n",
    "\n",
    "<img src=\"./fig/fig8.png\" style='height:500px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Neural Networks for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Networks Regression\n",
    "\n",
    "**Data:** features `x_train`, real-valued labels `y_train`\n",
    "\n",
    "**Probabilistic Model:** `y_train` $= g_\\mathbf{W}($ `x_train` $) + \\epsilon,\\quad$ $\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)$, where $g_\\mathbf{W}$ is a neural network with parameters $\\mathbf{W}$.\n",
    "\n",
    "**Training Objective:** find $\\mathbf{W}$ to maximize the likelihood of our data. This is equivalent to minimizing the Mean Square Error,\n",
    "$$\n",
    "\\max_{\\mathbf{W}}\\, \\mathrm{MSE}(\\mathbf{W}) = \\frac{1}{N}\\sum^N_{n=1} \\left(y_n - g_\\mathbf{W}(x_n)\\right)^2\n",
    "$$\n",
    "\n",
    "**Optimizing the Training Objective:** For linear regression (when $g_\\mathbf{W}$ is a linear function), we computed the gradient of the MSE with respective to the model parameters $\\mathbf{W}$, set it equal to zero and solved for the optimal $\\mathbf{W}$ analytically. \n",
    "\n",
    "Can we do the same when $g_\\mathbf{W}$ is a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Exercise: Optimizing Neural Networks\n",
    "\n",
    "For the small neural network in the previous exercise, compute the gradient $\\nabla_{\\mathbf{W}}\\,\\mathrm{MSE}(\\mathbf{W})$. Can you analytically solve for the optimal parameters $\\mathbf{W}$? Is the training objective convex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Your NN --> Gradient descent\n",
    "\n",
    "* Optimization Choices Matter\n",
    "\n",
    "<img src=\"./fig/fig10.jpg\" style='height:400px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gradient Descent: the Algorithm\n",
    "1. start at random place: $W_0\\leftarrow \\textbf{random}$\n",
    "\n",
    "2. until (stopping condition satisfied):\n",
    "\n",
    "  a. compute gradient: \n",
    "     gradient = $\\nabla$ loss\\_function($W_{t}$)\n",
    "\n",
    "  b. take a step in the negative gradient direction: \n",
    "     $W_{t+1} \\leftarrow W_{t}$ - eta * gradient\n",
    "\n",
    "Here *eta* is called the ***learning rate***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Drawbacks of Gradient Descent\n",
    "\n",
    "* Consider minimizing an average of functions (over examples):\n",
    "$$\n",
    "\\min_{x} \\frac{1}{N} \\sum_{i=1}^N f_W(x_i)\n",
    "$$\n",
    "\n",
    "* The gradient update looks:\n",
    "$$\n",
    "W_{t+1} \\leftarrow W_{t} - \\eta * \\sum_i \\frac{1}{N} \\nabla_W  f_W(x_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* What happens if you have a 10k examples? 100k examples? a million examples?\n",
    "* Is gradient descent still feasible?\n",
    "* How can we optimize over such database?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent\n",
    "\n",
    "* We could try to use less examples to approximate the gradient..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* With a single example:\n",
    "$$\n",
    "W_{t+1} \\leftarrow W_{t} - \\eta * \\nabla_W  f_{W_t}(x_i),\\qquad \\forall t = 1,2,\\dots\n",
    "$$\n",
    "* Every time we make an update on $W_t$, we take a new example randomly.\n",
    "* How do you think this affects the optimization procedure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"./fig/sgd.png\" style='height:450px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mini-batch gradient descent\n",
    "\n",
    "* Nobody calls it mini-batch gradient descent: SGD\n",
    "\n",
    "* Instead of using a single example --> use a mini-batch!\n",
    "$$\n",
    "W_{t+1} \\leftarrow W_{t} - \\eta * \\frac{1}{M} \\sum_{i=1}^M \\nabla_W  f_{W_t}(x_i),\\qquad \\forall t = 1,2,\\dots\n",
    "$$\n",
    "\n",
    "* This will reduce the variance of the updates.\n",
    "\n",
    "* Mini-batch size is another tuning parameter.\n",
    "\n",
    "* **Epoch:** A whole loop over all data samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Implementing Neural Networks in `python`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## `keras`: a Python Library for Neural Networks\n",
    "\n",
    "`keras` is a `python` library that provides intuitive api's for build neural networks quickly. \n",
    "\n",
    "``` python\n",
    "#keras model for feedforward neural networks\n",
    "from keras.models import Sequential\n",
    "\n",
    "#keras model for layers in feedforward networks\n",
    "from keras.layers import Dense\n",
    "\n",
    "#keras model for optimizing training objectives\n",
    "from keras import optimizers\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a Neural Network for Regression in `keras`\n",
    "\n",
    "`keras` is a `python` library that provides intuitive api's for build neural networks quickly.\n",
    "\n",
    "``` python\n",
    "#instantiate a feedforward model\n",
    "model = Sequential()\n",
    "\n",
    "#add layers sequentially\n",
    "\n",
    "#input layer: 2 input dimensions\n",
    "model.add(Dense(2, input_dim=2, activation='relu', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "#hidden layer: 2 nodes\n",
    "model.add(Dense(2, activation='relu', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "\n",
    "#output layer: 1 output dimension\n",
    "model.add(Dense(1, activation='relu', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "\n",
    "#configure the model: specify training objective and training algorithm\n",
    "adam = optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=adam,\n",
    "              loss='mean_squared_error')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing an optimizer\n",
    "\n",
    "* Adjust the learning rate dynamically.\n",
    "* Average of gradients.\n",
    "* Accelerated versions: increase convergence speed but also variance\n",
    "\n",
    "<img src=\"./fig/adam_sgd.png\" style='height:450px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a Neural Network in `keras`\n",
    "\n",
    "``` python\n",
    "#fit the model and return the mean squared error during training\n",
    "history = model.fit(X_train, Y_train, batch_size=20, shuffle=True, epochs=100, verbose=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Monitoring Neural Network Training\n",
    "\n",
    "Visualize the mean square error over the training, this is called the training ***trajectory***.\n",
    "\n",
    "``` python\n",
    "#fit the model and return the mean squared error during training\n",
    "history = model.fit(X_train, Y_train, batch_size=20, shuffle=True, epochs=100, verbose=0)\n",
    "\n",
    "# Plot the loss function and the evaluation metric over the course of training\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "\n",
    "ax.plot(np.array(history.history['mean_squared_error']), color='blue', label='training accuracy')\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Bias Variance Trade-off: for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization Error and Bias/Variance\n",
    "Complex models have ***low bias*** -- they can model a wide range of functions, given enough samples.\n",
    "\n",
    "But complex models like neural networks can use their 'extra' capacity to explain non-meaningful features of the training data that are unlikely to appear in the test data (i.e. noise). These models have ***high variance*** -- they are very sensitive to small changes in the data distribution, leading to drastic performance decrease from train to test settings.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"./fig/fig11.png\" style=\"width: 350px;\" align=\"center\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"./fig/fig12.png\" style=\"width: 350px;\" align=\"center\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization\n",
    "A way to prevent overfitting is to reduce the capacity of the model, thereby limiting the kinds of functions they can model. This **increases bias, but reduces variance**:\n",
    "1. **$\\ell_1$, $\\ell_2$ weight regularization** - adding a term to the loss function that penalizes the $\\ell_1$-norm (sum of absolute values) or the $\\ell_2$-norm (sum of squares) of the weights. This prevents the network from learning extremely squiggly functions.\n",
    "``` python\n",
    "from keras import regularizers\n",
    "model.add(Dense(64, input_dim=64,\n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "```\n",
    "\n",
    "2. **Dropout** - randomly zeroing out weights during training. This prevents the hidden nodes from \"over specializing\" or \"memorizing\" certain data points.\n",
    "``` python\n",
    "from keras.layers import Dropout,\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"./fig/dropout1.png\" style='height:450px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img src=\"./fig/dropout2.png\" style='height:450px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Compare Neural Network Regression to Polynomial Regression\n",
    "\n",
    "Compare a neural network regression to a polynomial regression. Which model do you think is \"better\" and for what kind of tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using Neural Networks for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Data:** features `x_train`, real-valued labels `y_train`\n",
    "\n",
    "**Probabilistic Model:** `y_train` $\\sim \\text{Bernoulli}(\\sigma(g_\\mathbf{W}($ `x_train` $))$, where $g_\\mathbf{W}$ is a neural network with parameters $\\mathbf{W}$, and $\\sigma(z) = \\frac{1}{1+e^{-z}}$.\n",
    "\n",
    "**Training Objective:** find $\\mathbf{W}$ to maximize the likelihood of our data. This is equivalent to minimizing the ***binary cross entropy*** or ***log loss***,\n",
    "$$\n",
    "\\max_{\\mathbf{W}}\\, \\mathrm{CrossEnt}(\\mathbf{W}) =\\sum^N_{n=1} y_n \\log( g_\\mathbf{W}(x_n)) + (1 - y_n) \\log\\left(1 - g_\\mathbf{W}(x_n)\\right)\n",
    "$$\n",
    "\n",
    "**Optimizing the Training Objective:** Since this objective is not convex and finding the zero's of the gradient is intractable, we will use gradient descent to find a \"optimal\" set of parameters $\\mathbf{W}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a Neural Network for Classification in `keras`\n",
    "\n",
    "``` python\n",
    "#instantiate a feedforward model\n",
    "model = Sequential()\n",
    "\n",
    "#add layers sequentially\n",
    "\n",
    "#input layer: 2 input dimensions\n",
    "model.add(Dense(2, input_dim=2, activation='relu', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "#hidden layer: 2 nodes\n",
    "model.add(Dense(2, activation='relu', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "\n",
    "#output layer: 1 output dimension\n",
    "model.add(Dense(1, activation='sigmoid', \n",
    "                kernel_initializer='random_uniform',\n",
    "                bias_initializer='zeros')) \n",
    "\n",
    "#configure the model: specify training objective and training algorithm\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='binary_crossentropy')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Reminder of bias & variance trade-off in classification\n",
    "\n",
    "<img src=\"./fig/bias_variance.png\" style='height:450px;'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Compare Neural Network Classification to Other Classifiers\n",
    "\n",
    "Compare a neural network classifier to other classifiers. For what task is a neural network the most appropriate model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KEY IDEAS recap:\n",
    "\n",
    "* NNs are universal approximators: can approximate ANY function with enough hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* How to we train NN? Gradient descent + chain rule = Back-propagation.\n",
    "\n",
    "* Wide variety of optimizers --> optimization alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Deep NNs can overfit, they WILL overfit!\n",
    "\n",
    "* Lot's of parameters to tune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Extra note**: Linear/Logistic Regression ARE neural networks with no hidden layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to avoid overfitting?\n",
    "- more training data\n",
    "- L2/L1 penalties on weights\n",
    "- data augmentation\n",
    "- dropout\n",
    "- early stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Optimizing quality/speed\n",
    "- learning rate\n",
    "- batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Orthogonalization\n",
    "\n",
    "<img src=\"./fig/orthogonalization.png\" style='height:450px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data augmentation\n",
    "\n",
    "<img src=\"./fig/data_aug.png\" style='height:450px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Early stopping\n",
    "\n",
    "<img src=\"./fig/early_stop.png\" style='height:450px;'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary advantages/disadvantages:\n",
    "\n",
    "* PROs:\n",
    "    - flexible models\n",
    "    - high performance (state-of-the-art: language, speech, images)\n",
    "    - open-source software/several ressources\n",
    "\n",
    "* CONs:\n",
    "    - require lots of data\n",
    "    - many tuning params\n",
    "    - computationally expensive\n",
    "    - optimization not easy: will it converge? is local minimum enough?\n",
    "    - will it overfit?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
