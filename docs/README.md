# Introduction to Machine Learning and Computational Statistics -- DSC6135

**University of Rwanda**

## Course Description

This course will give you an introduction to machine learning, probabilistic modeling and data science. We will cover two major areas in machine learning: supervised learning, and unsupervised learning.

Our learning approach will be a mixture of conceptual, theoretical, and practical. We will discuss the motivations behind common probabilistic models, and the properties that determine whether or not such models will work well for a particular task. On the one hand, you will derive the mathematical underpinnings for many common ML approaches, as well as apply those techniques to model real data.

## Quizzes

* 28 June: [regression](https://docs.google.com/forms/d/1gdUIEedv8AQIRTu1uKUPib9eMmp_5Vl2vj0GAfipMaI/edit).

## Schedule

|  **Date** | **Assignments** | **Lecture<br/>Topics and Slides** | **Practicals** | **Supplemental,<br/> Readings and Demos** |
| :---: | :---: | :---: | :---: | :---: |
|  27/June/2019 | [HW0](hw/hw0.md) | [00_intro to ML](slides/00_intro_slides.pdf) [01_regression](slides/01_regression.pdf) |  | [intro_numpy.ipynb](supplementary/intro_numpy.ipynb)<br/> [00_review_notes.pdf](slides/00_review_notes.pdf) |
|  28/June/2019 | [HW1](hw/hw1.md) | [02_model_evaluation.pdf](slides/02_model_evaluation.pdf)  |  [02_variance_reduction.ipynb](supplementary/02_practical_variance_reduction_and_likelihood.ipynb) | |
|  1/July/2019 |  | 03_bayesian_regression |  | |
|  2/July/2019 | out: HW2<br/> due: HW1 (midnight) | 04_classification.ipynb |  |  |
|  3/July/2019 |  | 05_neural_networks.ipynb |  | |
|  4/July/2019 |  | hackathon (6-10pm) |  |  |
|  5/July/2019 | out: HW3<br/> due: HW2 (midnight) | 06_kernels.ipynb |  |  |
|  8/July/2019 | out: HW4<br/> due: HW3 (midnight) | 07_dim_reduction.ipynb |  |  |
|  9/July/2019 |  | 08_clustering.ipynb |  |  |
|  10/July/2019 | out: HW5<br/> (+) due:HW4 (midnight) | 09_reco_systems.ipynb |  |  |
|  11/July/2019 |  | 10_topic_models.ipynb |  |  |
|  12/July/2019 | Presentation | 11_advanced_topics.ipynb<br/> ethics.pdf | | <https://learngitbranching.js.org/> |
|   | (+) due: HW5 on 14/July/2019 midnight |  |  |  |



## Goals
After this course, you will be able to...

- learn how to think in principled ways of modeling (understand the why's)
- understand deeply how and why machine learning works
- learn how to regularize models
- learn how to optimize objective functions
- learn how to validate
- deal with data computationally large/small, and statistically small.

## Prerequisites

- Python programming.
- Background in stats, and probability (we will review concepts).
- Some linear algebra, multivariate calculus.

Students are expected to write non-trivial programs. Code will be provided in python.


## Course Logistics

This is an intensive course of 4h/day for 10 days in total. Each day will be approximately structured as follows:

* Quizz of concepts/correction of homeworks + summary of previous day and questions (30 min)
* Lecture (1h)
* Practical (30 min)
* Break (15 min)
* Lecture (1h)
* Practical (30 min)
* Introduction to Homework (15 min)


Homeworks will be released/described at the end of class, every two days.
There will be short quizzes/review of concepts at the beginning of each day.


## Requirements and Grading

- (50 pts) 5 mandatory homeworks (every 2 days)
- (30 pts) quizzes
- (15 pts) paper presentation
- (5 pts) participation

Your main deliverable will be homework reports. You’ll be assessed on effort, the sophistication of your technical approach, the clarity of your explanations, the evidence that you present to support your evaluative claims, and the performance of your implementation. A high performing approach with little explanation will receive little credit, while a careful set of experiments that illuminate why a particular direction turned out to be a dead end may
receive close to full credit.


## Reference materials

- Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. [The elements of statistical learning](https://web.stanford.edu/~hastie/ElemStatLearn/). Vol. 1. No. 10. New York: Springer series in statistics, 2001.
- John Winn, Christopher M. Bishop, Thomas Diethe, John Guiver and Yordan Zaykov. [Model based machine learning](http://www.mbmlbook.com). 2013.
- Bishop, Christopher M. Pattern recognition and machine learning. Springer, 2006.
- Murphy, Kevin P. Machine learning: a probabilistic perspective. MIT press, 2012.


## Philosophy

The goal of this course is to instill a strong technical background for you to responsibly apply machine learning in the world. Thus, in addition to the derivations and the practicals, each class will include a story about real-world applications of machine learning. We will also talk about ethical implications of machine learning.

Related, we expect all participants in this course—instructors, staff, students—to be
committed to an open, professional, and inclusive environment. Just like the maths, these qualities take cultivation and effort. We will start with the premise that we are all open-minded people trying our best and encourage constructive feedback to improving the course environment.

## Acknowledgement

Many slides and homeworks are attributable to/inspired by:
* Mike Hughes (Tufts University)
* Finale Doshi-Velez (Harvard University)
* Erik Sudderth (University of California)
* James, Witten, Hastie, Tibshirani, Bishop (ISL/ESL books)
